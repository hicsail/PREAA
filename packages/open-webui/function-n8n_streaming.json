[{"id":"n8n_streaming","user_id":"5e6f9767-568e-4a08-a905-54d9d229049d","name":"n8n Sreaming","type":"pipe","content":"\"\"\"\ntitle: n8n Streaming\nauthor: James @ foxbyte.tech (inspired by owndev and patched by j3hn)\nauthor_url: https://github.com/webfox/\nn8n_template: https://github.com/owndev/Open-WebUI-Functions/blob/master/pipelines/n8n/Open_WebUI_Test_Agent.json\nversion: 1.0.0\nlicense: Apache License 2.0\ndescription: A pipeline for interacting with N8N workflows with full streaming support. Seamlessly connects Open WebUI to N8N AI agents and workflows.\nfeatures:\n  - Real-time streaming responses from N8N workflows\n  - Filters out N8N metadata for clean output\n  - Encrypted storage of sensitive API keys\n  - Configurable status emissions\n  - Supports both streaming and non-streaming N8N workflows\n\"\"\"\n\nimport time\nimport aiohttp\nimport os\nimport base64\nimport hashlib\nimport logging\nimport json\nfrom typing import Optional, Callable, Awaitable, Any, Dict\nfrom pydantic import BaseModel, Field, GetCoreSchemaHandler\nfrom cryptography.fernet import Fernet, InvalidToken\nfrom open_webui.env import SRC_LOG_LEVELS\nfrom pydantic_core import core_schema\n\n\nclass EncryptedStr(str):\n    \"\"\"A string type that automatically handles encryption/decryption\"\"\"\n\n    @classmethod\n    def _get_encryption_key(cls) -> Optional[bytes]:\n        \"\"\"Generate encryption key from WEBUI_SECRET_KEY environment variable\"\"\"\n        secret = os.getenv(\"WEBUI_SECRET_KEY\")\n        if not secret:\n            return None\n        hashed_key = hashlib.sha256(secret.encode()).digest()\n        return base64.urlsafe_b64encode(hashed_key)\n\n    @classmethod\n    def encrypt(cls, value: str) -> str:\n        \"\"\"Encrypt a string value\"\"\"\n        if not value or value.startswith(\"encrypted:\"):\n            return value\n        key = cls._get_encryption_key()\n        if not key:\n            return value\n        f = Fernet(key)\n        encrypted = f.encrypt(value.encode())\n        return f\"encrypted:{encrypted.decode()}\"\n\n    @classmethod\n    def decrypt(cls, value: str) -> str:\n        \"\"\"Decrypt an encrypted string value\"\"\"\n        if not value or not value.startswith(\"encrypted:\"):\n            return value\n        key = cls._get_encryption_key()\n        if not key:\n            return value[len(\"encrypted:\") :]\n        try:\n            encrypted_part = value[len(\"encrypted:\") :]\n            f = Fernet(key)\n            decrypted = f.decrypt(encrypted_part.encode())\n            return decrypted.decode()\n        except (InvalidToken, Exception):\n            return value\n\n    @classmethod\n    def __get_pydantic_core_schema__(\n        cls, _source_type: Any, _handler: GetCoreSchemaHandler\n    ) -> core_schema.CoreSchema:\n        return core_schema.union_schema(\n            [\n                core_schema.is_instance_schema(cls),\n                core_schema.chain_schema(\n                    [\n                        core_schema.str_schema(),\n                        core_schema.no_info_plain_validator_function(\n                            lambda value: cls(cls.encrypt(value) if value else value)\n                        ),\n                    ]\n                ),\n            ],\n            serialization=core_schema.plain_serializer_function_ser_schema(\n                lambda instance: str(instance)\n            ),\n        )\n\n    @classmethod\n    def get_decrypted(self) -> str:\n        \"\"\"Get the decrypted value of this encrypted string\"\"\"\n        return self.decrypt(self)\n\n\nclass Pipe:\n    class Valves(BaseModel):\n        N8N_URL: str = Field(\n            default=\"https://your-n8n-instance.com/webhook/your-webhook-id\",\n            description=\"URL for the N8N webhook endpoint\",\n        )\n        N8N_BEARER_TOKEN: EncryptedStr = Field(\n            default=\"\",\n            description=\"Bearer token for authenticating with the N8N webhook (optional)\",\n        )\n        INPUT_FIELD: str = Field(\n            default=\"chatInput\",\n            description=\"Field name for the input message in the N8N payload\",\n        )\n        RESPONSE_FIELD: str = Field(\n            default=\"output\",\n            description=\"Field name for the response message in non-streaming N8N responses\",\n        )\n        EMIT_INTERVAL: float = Field(\n            default=1.0, description=\"Interval in seconds between status emissions\"\n        )\n        ENABLE_STATUS_INDICATOR: bool = Field(\n            default=True, description=\"Enable or disable status indicator emissions\"\n        )\n\n    def __init__(self):\n        self.name = \"N8N Pipeline\"\n        self.valves = self.Valves()\n        self.last_emit_time = 0\n        self.log = logging.getLogger(\"n8n_pipeline\")\n        self.log.setLevel(SRC_LOG_LEVELS.get(\"N8N\", logging.INFO))\n\n    async def emit_status(\n        self,\n        event_emitter: Optional[Callable[[dict], Awaitable[None]]],\n        level: str,\n        message: str,\n        done: bool = False,\n    ) -> None:\n        \"\"\"Emit status updates to Open WebUI\"\"\"\n        if not event_emitter or not self.valves.ENABLE_STATUS_INDICATOR:\n            return\n\n        current_time = time.time()\n        if current_time - self.last_emit_time >= self.valves.EMIT_INTERVAL or done:\n            await event_emitter(\n                {\n                    \"type\": \"status\",\n                    \"data\": {\n                        \"status\": \"complete\" if done else \"in_progress\",\n                        \"level\": level,\n                        \"description\": message,\n                        \"done\": done,\n                    },\n                }\n            )\n            self.last_emit_time = current_time\n\n    def extract_event_info(\n        self, event_emitter: Optional[Callable]\n    ) -> tuple[Optional[str], Optional[str]]:\n        \"\"\"Extract chat_id and message_id from event emitter closure\"\"\"\n        if (\n            not event_emitter\n            or not hasattr(event_emitter, \"__closure__\")\n            or not event_emitter.__closure__\n        ):\n            return None, None\n\n        for cell in event_emitter.__closure__:\n            if hasattr(cell, \"cell_contents\") and isinstance(cell.cell_contents, dict):\n                request_info = cell.cell_contents\n                return request_info.get(\"chat_id\"), request_info.get(\"message_id\")\n        return None, None\n\n    def get_headers(self) -> Dict[str, str]:\n        \"\"\"Build HTTP headers for N8N request\"\"\"\n        headers = {\"Content-Type\": \"application/json\"}\n\n        bearer_token = EncryptedStr.decrypt(self.valves.N8N_BEARER_TOKEN)\n        if bearer_token:\n            headers[\"Authorization\"] = f\"Bearer {bearer_token}\"\n\n        return headers\n\n    def parse_n8n_streaming_chunk(self, chunk_text: str) -> Optional[str]:\n        \"\"\"Parse N8N streaming chunk and extract content, filtering out metadata\"\"\"\n        if not chunk_text.strip():\n            return None\n\n        try:\n            data = json.loads(chunk_text.strip())\n\n            if isinstance(data, dict):\n                # Skip N8N metadata chunks\n                chunk_type = data.get(\"type\", \"\")\n                if chunk_type in [\"begin\", \"end\", \"error\", \"metadata\"]:\n                    self.log.debug(f\"Skipping N8N metadata chunk: {chunk_type}\")\n                    return None\n\n                # Skip metadata-only chunks\n                if \"metadata\" in data and len(data) <= 2:\n                    return None\n\n                # Extract content from various possible field names\n                content = (\n                    data.get(\"text\")\n                    or data.get(\"content\")\n                    or data.get(\"output\")\n                    or data.get(\"message\")\n                    or data.get(\"delta\")\n                    or data.get(\"data\")\n                )\n\n                # Handle OpenAI-style streaming format\n                if not content and \"choices\" in data:\n                    choices = data.get(\"choices\", [])\n                    if choices and isinstance(choices[0], dict):\n                        delta = choices[0].get(\"delta\", {})\n                        content = delta.get(\"content\", \"\")\n\n                if content:\n                    return str(content)\n\n                # Return non-metadata objects as strings\n                if not any(\n                    key in data for key in [\"type\", \"metadata\", \"nodeId\", \"nodeName\"]\n                ):\n                    return str(data)\n\n        except json.JSONDecodeError:\n            # Handle plain text content\n            if not chunk_text.startswith(\"{\"):\n                return chunk_text.strip()\n\n        return None\n\n    def extract_content_from_mixed_stream(self, raw_text: str) -> str:\n        \"\"\"Extract content from mixed stream containing both metadata and content\"\"\"\n        content_parts = []\n\n        # Handle concatenated JSON objects\n        parts = raw_text.split(\"}{\")\n\n        for i, part in enumerate(parts):\n            # Reconstruct valid JSON\n            if i > 0:\n                part = \"{\" + part\n            if i < len(parts) - 1:\n                part = part + \"}\"\n\n            extracted = self.parse_n8n_streaming_chunk(part)\n            if extracted:\n                content_parts.append(extracted)\n\n        return \"\".join(content_parts)\n\n    def build_payload(\n        self,\n        messages: list,\n        user: Optional[dict],\n        chat_id: Optional[str],\n        message_id: Optional[str],\n    ) -> dict:\n        \"\"\"Build the payload for N8N request\"\"\"\n        if not messages:\n            return {}\n\n        # Extract the current user's question\n        question = messages[-1][\"content\"]\n        if \"Prompt: \" in question:\n            question = question.split(\"Prompt: \")[-1]\n\n        # Extract system prompt if available\n        system_prompt = \"\"\n        if messages and messages[0].get(\"role\") == \"system\":\n            system_content = messages[0][\"content\"]\n            if \"Prompt: \" in system_content:\n                system_prompt = system_content.split(\"Prompt: \")[-1]\n            else:\n                system_prompt = system_content\n\n        # **NEW: Include full conversation history**\n        conversation_history = []\n        for msg in messages:\n            if msg.get(\"role\") in [\"user\", \"assistant\"]:\n                conversation_history.append(\n                    {\"role\": msg[\"role\"], \"content\": msg[\"content\"]}\n                )\n\n        payload = {\n            \"systemPrompt\": system_prompt,\n            \"messages\": conversation_history,  # **ADD THIS LINE**\n            \"currentMessage\": question,  # **ADD THIS LINE**\n            \"chat_id\": chat_id,\n            \"message_id\": message_id,\n        }\n\n        # Add user information if available\n        if user:\n            payload.update(\n                {\n                    \"user_id\": user.get(\"id\"),\n                    \"user_email\": user.get(\"email\"),\n                    \"user_name\": user.get(\"name\"),\n                    \"user_role\": user.get(\"role\"),\n                }\n            )\n\n        # Keep the original field for backward compatibility\n        payload[self.valves.INPUT_FIELD] = question\n\n        return payload\n\n    def extract_non_streaming_response(self, response_data: Any) -> str:\n        \"\"\"Extract content from non-streaming N8N response\"\"\"\n        # Handle array responses (common with \"Respond to Webhook\" + \"allIncomingItems\")\n        if isinstance(response_data, list) and response_data:\n            first_item = response_data[0]\n            if isinstance(first_item, dict):\n                return (\n                    first_item.get(self.valves.RESPONSE_FIELD)\n                    or first_item.get(\"text\")\n                    or first_item.get(\"content\")\n                    or first_item.get(\"output\")\n                    or str(first_item)\n                )\n            return str(first_item)\n\n        # Handle dict responses\n        if isinstance(response_data, dict):\n            return (\n                response_data.get(self.valves.RESPONSE_FIELD)\n                or response_data.get(\"text\")\n                or response_data.get(\"content\")\n                or response_data.get(\"output\")\n                or str(response_data)\n            )\n\n        return str(response_data)\n\n    async def pipe(\n        self,\n        body: dict,\n        __user__: Optional[dict] = None,\n        __event_emitter__: Optional[Callable[[dict], Awaitable[None]]] = None,\n        __event_call__: Optional[Callable[[dict], Awaitable[dict]]] = None,\n    ) -> str:\n        \"\"\"Main pipeline function\"\"\"\n\n        await self.emit_status(__event_emitter__, \"info\", f\"Thinking...\")\n\n        messages = body.get(\"messages\", [])\n        if not messages:\n            error_msg = \"No messages found in the request body\"\n            self.log.warning(error_msg)\n            await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n            return error_msg\n\n        n8n_response = \"\"\n\n        try:\n            # Extract request information\n            chat_id, message_id = self.extract_event_info(__event_emitter__)\n\n            # Build request payload\n            payload = self.build_payload(messages, __user__, chat_id, message_id)\n            headers = self.get_headers()\n\n            self.log.info(f\"Sending request to N8N: {self.valves.N8N_URL}\")\n            await self.emit_status(__event_emitter__, \"info\", \"Thinking...\")\n\n            async with aiohttp.ClientSession(\n                trust_env=True,\n                timeout=aiohttp.ClientTimeout(total=None),\n            ) as session:\n                async with session.post(\n                    self.valves.N8N_URL, json=payload, headers=headers\n                ) as response:\n\n                    if response.status != 200:\n                        error_text = await response.text()\n                        raise Exception(\n                            f\"N8N returned HTTP {response.status}: {error_text}\"\n                        )\n\n                    content_type = response.headers.get(\"Content-Type\", \"\").lower()\n                    is_streaming = (\n                        \"stream\" in content_type\n                        or \"text/plain\" in content_type\n                        or response.headers.get(\"Transfer-Encoding\") == \"chunked\"\n                    )\n\n                    if is_streaming:\n                        # --- STREAMING MODE ---\n                        self.log.info(\"Processing streaming response from N8N\")\n                        buffer = \"\"\n\n                        async for chunk in response.content.iter_any():\n                            if not chunk:\n                                continue\n\n                            text = chunk.decode(errors=\"ignore\")\n                            buffer += text\n\n                            # Process complete JSON objects\n                            while True:\n                                start_idx = buffer.find(\"{\")\n                                if start_idx == -1:\n                                    break\n\n                                # Find matching closing brace\n                                brace_count = 0\n                                end_idx = -1\n\n                                for i in range(start_idx, len(buffer)):\n                                    if buffer[i] == \"{\":\n                                        brace_count += 1\n                                    elif buffer[i] == \"}\":\n                                        brace_count -= 1\n                                        if brace_count == 0:\n                                            end_idx = i\n                                            break\n\n                                if end_idx == -1:\n                                    # Incomplete JSON, wait for more data\n                                    break\n\n                                # Extract and process the JSON chunk\n                                json_chunk = buffer[start_idx : end_idx + 1]\n                                buffer = buffer[end_idx + 1 :]\n\n                                content = self.parse_n8n_streaming_chunk(json_chunk)\n                                if content:\n                                    n8n_response += content\n\n                                    if __event_emitter__:\n                                        await __event_emitter__(\n                                            {\n                                                \"type\": \"chat:message:delta\",\n                                                \"data\": {\n                                                    \"role\": \"assistant\",\n                                                    \"content\": content,\n                                                },\n                                            }\n                                        )\n\n                        # Process any remaining content in buffer\n                        if buffer.strip():\n                            remaining_content = self.extract_content_from_mixed_stream(\n                                buffer\n                            )\n                            if remaining_content:\n                                n8n_response += remaining_content\n                                if __event_emitter__:\n                                    await __event_emitter__(\n                                        {\n                                            \"type\": \"chat:message:delta\",\n                                            \"data\": {\n                                                \"role\": \"assistant\",\n                                                \"content\": remaining_content,\n                                            },\n                                        }\n                                    )\n\n                        # Emit final complete message\n                        if n8n_response and __event_emitter__:\n                            await __event_emitter__(\n                                {\n                                    \"type\": \"chat:message\",\n                                    \"data\": {\n                                        \"role\": \"assistant\",\n                                        \"content\": n8n_response,\n                                    },\n                                }\n                            )\n                    else:\n                        # --- NON-STREAMING MODE ---\n                        self.log.info(\"Processing non-streaming response from N8N\")\n\n                        try:\n                            response_data = await response.json()\n                            n8n_response = self.extract_non_streaming_response(\n                                response_data\n                            )\n                        except json.JSONDecodeError:\n                            # Fall back to text response\n                            raw_text = await response.text()\n                            n8n_response = (\n                                self.extract_content_from_mixed_stream(raw_text)\n                                or raw_text\n                            )\n\n        except Exception as e:\n            error_msg = f\"Error: {str(e)}\"\n            self.log.exception(error_msg)\n            await self.emit_status(__event_emitter__, \"error\", error_msg, True)\n            return error_msg\n\n        # Update conversation with response\n        body[\"messages\"].append({\"role\": \"assistant\", \"content\": n8n_response})\n        await self.emit_status(__event_emitter__, \"info\", \"Thinking complete\", True)\n\n        return n8n_response\n","meta":{"description":"Seamlessly connect Open WebUI to N8N workflows with real-time streaming support, enabling powerful AI agents that can access external APIs, databases, and services while filtering metadata for clean conversation flow.","manifest":{"title":"n8n Streaming","author":"James @ foxbyte.tech (inspired by owndev and patched by j3hn)","author_url":"https://github.com/webfox/","version":"1.0.0","license":"Apache License 2.0","description":"A pipeline for interacting with N8N workflows with full streaming support. Seamlessly connects Open WebUI to N8N AI agents and workflows.","features":""},"type":"pipe","user":{"id":"c7883ae5-a701-4221-9415-2b1d666ccf78","username":"webfox","name":"James Simmonds","createdAt":1739566457,"role":null,"verified":false},"id":"f850c656-13e0-45cf-92f9-6c7154a029c7"},"is_active":true,"is_global":false,"updated_at":1763568656,"created_at":1762451034}]
